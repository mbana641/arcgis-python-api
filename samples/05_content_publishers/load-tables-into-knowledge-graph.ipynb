{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e209cc",
   "metadata": {},
   "source": [
    "## This cell will contain an introduction to the process that this notebook will outline and why I am writing it\n",
    "\n",
    "Loading tables of data into a knowledge graph is a vital step to set up the graph for analysis. This process includes deciding on how you want to model the data, determining the types and properties that will be created, and loading data from all of your tables in to complete the graph.\n",
    "\n",
    "During this process, it's important to be able to merge data in order to ensure there are not duplicates of the same entities or relationships created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20af993",
   "metadata": {},
   "source": [
    "## This cell will contain information about the dataset that we will be using : https://github.com/biocypher/pole/blob/main/data/pole.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8149bb",
   "metadata": {},
   "source": [
    "## This cell will contain information about what the same process would look like in Load Table in Pro and highlight the specific processes we are doing in this notebook that aren't currently easily accessible in the python API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcec6ca",
   "metadata": {},
   "source": [
    "### To begin, we will need to import the libraries that are used in this notebook\n",
    "We will be importing the os and json libraries for dealing with our input tables and manipulating data. The pandas library will be used for cleaning and preparing the csvs that we will be using before passing them into the loading function. Finally, GIS is used to connect to our portal and create the service we will be using and GraphClient along with the following classes will allow us to create the correct structures easily for creating the data model and data for our knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f506d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json, os, uuid\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from arcgis.gis import GIS\n",
    "from arcgis.geometry import Point\n",
    "from arcgis.graph import GraphClient, EntityType, RelationshipType, GraphProperty, GraphObject, Entity, Relationship, SearchIndexProperties, data_model_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d987b760",
   "metadata": {},
   "source": [
    "## Set up wrappers to help us with creating correct loading configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860ae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input table for the type being loaded\n",
    "class InputTable:\n",
    "    def __init__(self, csv_path):\n",
    "        self.dataframe = pd.read_csv(csv_path)\n",
    "        uniqueids = []\n",
    "        for i in range(len(self.dataframe)):\n",
    "            uniqueids.append(uuid.uuid4())\n",
    "        self.dataframe[\"globalid\"] = uniqueids\n",
    "        for col in self.dataframe.columns:\n",
    "            if self.dataframe[col].dtype in ['string','object']:\n",
    "                self.dataframe[col].fillna(\"None\",inplace=True)\n",
    "            if self.dataframe[col].dtype in ['int','float','int64','float64']:\n",
    "                self.dataframe[col].fillna(-1,inplace=True)\n",
    "        \n",
    "    def merge(self, merge_columns:list):\n",
    "        self.dataframe.drop_duplicates(subset=merge_columns, inplace=True)\n",
    "\n",
    "# property for a load table type, includes everything necessary to load\n",
    "class LoadTableProperty:\n",
    "    def __init__(self, name: str, data_type: data_model_types.esriFieldType, column: str|list[str], merge: bool = False, geometry_type: data_model_types.esriGeometryType = None):\n",
    "        self.name = name\n",
    "        self.data_type = data_type\n",
    "        self.column = column\n",
    "        self.merge = merge\n",
    "        self.geometry_type = geometry_type\n",
    "\n",
    "# relationship endpoints, these will represent the origin and destination information for relationship types\n",
    "class RelationshipEndpoint:\n",
    "    def __init__(self, entity_type: str, property: str, column: str, column_in_type=None):\n",
    "        self.entity_type = entity_type\n",
    "        self.property = property\n",
    "        self.column = column\n",
    "        self.column_in_type = column_in_type\n",
    "\n",
    "# entity type to load\n",
    "class LoadTableEntityType:\n",
    "    def __init__(self, type_name: str, table: InputTable, properties: list[LoadTableProperty] = []):\n",
    "        self.type_name = type_name\n",
    "        self.named_object_type = \"entity\"\n",
    "        self.table = table\n",
    "        self.properties = properties\n",
    "        \n",
    "    def add_properties(self, property_adds=[]):\n",
    "        self.properties += property_adds\n",
    "        \n",
    "    def get_merge_columns(self):\n",
    "        merge_columns = []\n",
    "        for prop in self.properties:\n",
    "            if prop.merge==True:\n",
    "                merge_columns.append(prop.column)\n",
    "        return merge_columns\n",
    "    \n",
    "    def get_merge_properties(self):\n",
    "        merge_props = []\n",
    "        for prop in self.properties:\n",
    "            if prop.merge==True:\n",
    "                merge_props.append(prop.name)\n",
    "        return merge_props\n",
    "        \n",
    "# relationship type to load\n",
    "class LoadTableRelationshipType:\n",
    "    def __init__(self, type_name: str, table: InputTable, origin: RelationshipEndpoint, destination: RelationshipEndpoint, properties: list[LoadTableProperty] = []):\n",
    "        self.type_name = type_name\n",
    "        self.named_object_type = \"relationship\"\n",
    "        self.table = table\n",
    "        self.origin = origin\n",
    "        self.destination = destination\n",
    "        self.properties = properties\n",
    "        \n",
    "    def add_properties(self, property_adds=[]):\n",
    "        self.properties += property_adds\n",
    "        \n",
    "    def get_merge_columns(self):\n",
    "        merge_columns = []\n",
    "        for prop in self.properties:\n",
    "            if prop.merge==True:\n",
    "                merge_columns.append(prop.column)\n",
    "        return merge_columns\n",
    "    \n",
    "    def get_merge_properties(self):\n",
    "        merge_props = []\n",
    "        for prop in self.properties:\n",
    "            if prop.merge==True:\n",
    "                merge_props.append(prop.name)\n",
    "        return merge_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16bbb5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_type_to_data_model(knowledge_graph: GraphClient, input_type: LoadTableEntityType|LoadTableRelationshipType):\n",
    "    # determine data model types and properties to be created\n",
    "    # add extra properties to type if existing\n",
    "    existing_datamodel = knowledge_graph.query_data_model()\n",
    "    if (input_type.type_name in kg_datamodel.entity_types.keys() or input_type.type_name in kg_datamodel.relationship_types.keys()):\n",
    "        property_adds = []\n",
    "        existing_type = existing_datamodel.entity_types[input_type.type_name] if input_type.named_object_type == \"entity\" else existing_datamodel.relationship_types[input_type.type_name]\n",
    "        for prop in input_type.properties:\n",
    "            try:\n",
    "                existing_type.properties[prop.name]\n",
    "            except:\n",
    "                if prop.data_type == \"esriFieldTypeGeometry\":\n",
    "                    property_adds[prop] = GraphProperty(name=prop.name, field_type=prop.data_type, geometry_type=prop.geometry_type)\n",
    "                else:\n",
    "                    property_adds[prop] = GraphProperty(name=prop.name, field_type=prop.data_type)\n",
    "        # add extra properties if they are not on existing types, this is done now since it uses individual types\n",
    "        knowledge_graph.graph_property_adds(type_name=input_type.type_name, graph_properties=property_adds)\n",
    "    else:\n",
    "        type_to_add = EntityType(name=input_type.type_name) if input_type.named_object_type == \"entity\" else RelationshipType(name=input_type.type_name)\n",
    "        for prop in input_type.properties:\n",
    "            if prop.data_type == \"esriFieldTypeGeometry\":\n",
    "                type_to_add.properties[prop] = GraphProperty(name=prop.name, field_type=prop.data_type, geometry_type=prop.geometry_type)\n",
    "            else:\n",
    "                type_to_add.properties[prop] = GraphProperty(name=prop.name, field_type=prop.data_type)\n",
    "        return type_to_add\n",
    "        \n",
    "def process_type_to_instances(knowledge_graph: GraphClient, input_type: LoadTableEntityType|LoadTableRelationshipType, entities_for_relationships: list[LoadTableEntityType] = None):\n",
    "    editing_adds = []\n",
    "    editing_updates = []\n",
    "    merge_props = []\n",
    "    kg_datamodel = knowledge_graph.query_data_model()\n",
    "    for prop in input_type.properties:\n",
    "        if prop.merge==True:\n",
    "            merge_props.append(prop.name)\n",
    "    if ((merge_props != []) & (input_type.type_name in kg_datamodel.entity_types.keys() or input_type.type_name in kg_datamodel.relationship_types.keys())):\n",
    "        # get values for the merge columns, put them into lists and create a bind parameter for each column name\n",
    "        bind_params = {}\n",
    "        query_string = \"\"\n",
    "        merge_columns = input_type.get_merge_columns()\n",
    "        merge_properties = input_type.get_merge_properties()\n",
    "        for column,prop in zip(merge_columns,merge_properties):\n",
    "            bind_params[prop] = input_type.table.dataframe[column].tolist()\n",
    "        # create query streaming to graph to get any matches on the column/value combos\n",
    "        if input_type.named_object_type == \"entity\":\n",
    "            query_string = f\"MATCH (n:{input_type.type_name}) WHERE \"\n",
    "            for prop in merge_properties:\n",
    "                query_string += f'n.{prop} in ${prop} AND'\n",
    "            query_string = query_string[:-4] + f\" RETURN \"\n",
    "            for prop in merge_properties:\n",
    "                query_string += \"n.\"+prop+','\n",
    "            query_string += \" n.globalid\"\n",
    "        if input_type.named_object_type == \"relationship\":\n",
    "            query_string = f\"MATCH ()-[n:{input_type.type_name}]-() WHERE \"\n",
    "            for prop in merge_properties:\n",
    "                query_string += f'n.{prop} in ${prop} AND'\n",
    "            query_string = query_string[:-4] + f\" RETURN \"\n",
    "            for prop in merge_properties:\n",
    "                query_string += \"n.\"+prop+','\n",
    "            query_string += \" n.globalid\"\n",
    "        # get matching merge candidates\n",
    "        results = knowledge_graph.query(query_string, bind_param=bind_params)\n",
    "        try:\n",
    "            for match in list(results):\n",
    "                all_props = {}\n",
    "                column_params = None\n",
    "                for i,column in enumerate(merge_columns):\n",
    "                    if column_params == None:\n",
    "                        column_params = (input_type.table.dataframe[column] == match[i])\n",
    "                    else:\n",
    "                        column_params = column_params & (input_type.table.dataframe[column] == match[i])\n",
    "                globalid_to_drop = input_type.table.dataframe.loc[column_params]['globalid'].values[0]\n",
    "                row = input_type.table.dataframe.loc[input_type.table.dataframe['globalid']==globalid_to_drop]\n",
    "                for prop in input_type.properties:\n",
    "                    all_props[prop.name] = row[prop.column]\n",
    "                input_type.table.dataframe[input_type.table.dataframe['globalid'] != globalid_to_drop]\n",
    "                editing_updates.append(Entity(type_name=input_type.type_name,id=globalid_to_drop,properties=all_props))\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "    for index, row in input_type.table.dataframe.iterrows():\n",
    "        all_props = {}\n",
    "        for prop in input_type.properties:\n",
    "            if type(prop.column) == list:\n",
    "                all_props[prop.name] = Point({\"x\": row[prop.column[1]], \"y\": row[prop.column[0]], \"spatialReference\": {\"wkid\": 4326}})\n",
    "            else:\n",
    "                all_props[prop.name] = row[prop.column]\n",
    "        if input_type.named_object_type == \"entity\":\n",
    "            editing_adds.append(Entity(type_name=input_type.type_name, id=row['globalid'], properties=all_props))\n",
    "        else:\n",
    "            origin_id = None\n",
    "            destination_id = None\n",
    "            if input_type.origin.column_in_type != None:\n",
    "                for type_to_use in entities_for_relationships:\n",
    "                    if type_to_use.type_name == input_type.origin.entity_type:\n",
    "                        try:\n",
    "                            origin_id = type_to_use.table.dataframe.loc[type_to_use.table.dataframe[input_type.origin.column_in_type] == row[input_type.origin.column],'globalid'].values[0]\n",
    "                        except:\n",
    "                            continue\n",
    "            else:\n",
    "                result = knowledge_graph.query(f\"MATCH (n:{input_type.origin.type_name}) WHERE {input_type.origin.property} = {row[input_type.origin.column]} RETURN n.globalid\")\n",
    "                origin_id = list(result)[0][0]\n",
    "            if input_type.destination.column_in_type != None:\n",
    "                for type_to_use in entities_for_relationships:\n",
    "                    if type_to_use.type_name ==  input_type.destination.entity_type:\n",
    "                        try:\n",
    "                            destination_id = type_to_use.table.dataframe.loc[type_to_use.table.dataframe[input_type.destination.column_in_type] == row[input_type.destination.column],'globalid'].values[0]\n",
    "                        except:\n",
    "                            continue\n",
    "            else:\n",
    "                result = knowledge_graph.query(f\"MATCH (n:{input_type.destination.type_name}) WHERE {input_type.destination.property} = {row[input_type.destination.column]} RETURN n.globalid\")\n",
    "                destination_id = list(result)[0][0]\n",
    "            # TO DO: if either are still null, I should be creating new entities and using the ids\n",
    "            if origin_id == None:\n",
    "                origin_id = uuid.uuid4()\n",
    "                editing_adds.append(Entity(type_name=input_type.origin.entity_type, id=origin_id, properties={input_type.origin.property: row[input_type.origin.column]}))\n",
    "            if destination_id == None:\n",
    "                destination_id = uuid.uuid4()\n",
    "                editing_adds.append(Entity(type_name=input_type.destination.entity_type, id=destination_id, properties={input_type.destination.property: row[input_type.destination.column]}))\n",
    "            editing_adds.append(Relationship(type_name=input_type.type_name, id=row['globalid'], origin_entity_id=origin_id, destination_entity_id=destination_id, properties=all_props))\n",
    "    return [editing_adds, editing_updates]\n",
    "            \n",
    "def load_tables(knowledge_graph:GraphClient, add_entities: list[LoadTableEntityType], add_relationships: list[LoadTableRelationshipType]):\n",
    "    # set up variables\n",
    "    entity_type_adds = []\n",
    "    relationship_type_adds = []\n",
    "    all_editing_adds = []\n",
    "    all_editing_updates = []\n",
    "    \n",
    "    for current_type in add_entities:\n",
    "        # determine if merge needs to be performed on InputTable for type\n",
    "        merge_props = []\n",
    "        merge_columns = current_type.get_merge_columns()\n",
    "        if merge_props != []:\n",
    "            current_type.table.merge(merge_columns)\n",
    "            \n",
    "        # create all necessary new types in data and update existing types with extra properties\n",
    "        type_add = process_type_to_data_model(knowledge_graph, current_type)\n",
    "        entity_type_adds.append(type_add)\n",
    "        \n",
    "        # set up instances of all types to be created, assign and track globalids as we go to make loading easier and faster, do merging as needed\n",
    "        [editing_adds, editing_updates] = process_type_to_instances(knowledge_graph, current_type)\n",
    "        all_editing_adds += editing_adds\n",
    "        all_editing_updates += editing_updates\n",
    "    \n",
    "    for current_type in add_relationships:\n",
    "        # determine if merge needs to be performed on InputTable for type\n",
    "        merge_columns = []\n",
    "        for prop in current_type.properties:\n",
    "            if prop.merge==True:\n",
    "                merge_columns.append(prop.column)\n",
    "        if merge_props != []:\n",
    "            current_type.table.merge(merge_columns)\n",
    "            \n",
    "        # create all necessary new types in data and update existing types with extra properties\n",
    "        type_add = process_type_to_data_model(knowledge_graph, current_type)\n",
    "        relationship_type_adds.append(type_add)\n",
    "        \n",
    "        [editing_adds, editing_updates] = process_type_to_instances(knowledge_graph, current_type, add_entities)\n",
    "        all_editing_adds += editing_adds\n",
    "        all_editing_updates += editing_updates\n",
    "    # add all necessary types to the data model\n",
    "    knowledge_graph.named_object_type_adds(entity_types=entity_type_adds, relationship_types=relationship_type_adds)\n",
    "    if all_editing_adds != []:\n",
    "        response = knowledge_graph.apply_edits(adds=all_editing_adds)\n",
    "        if response.error:\n",
    "            print(\"there was an error in the edit: \"+str(response.error))\n",
    "    if all_editing_updates != []:\n",
    "        knowledge_graph.apply_edits(updates=all_editing_updates)\n",
    "        if response.error:\n",
    "            print(\"there was an error in the edit: \"+str(response.error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b86d144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_adds = [\n",
    "    LoadTableEntityType(\"Person\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Person.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"last_name\", \"esriFieldTypeString\", \"surname\", merge=True),\n",
    "        LoadTableProperty(\"nhs_no\", \"esriFieldTypeString\", \"nhs_no\"),\n",
    "        LoadTableProperty(\"first_name\", \"esriFieldTypeString\", \"name\", merge=True)\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Officer\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Officer.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge= True),\n",
    "        LoadTableProperty(\"badge_number\", \"esriFieldTypeString\", \"badge_no\"),\n",
    "        LoadTableProperty(\"rank\", \"esriFieldTypeString\", \"rank\"),\n",
    "        LoadTableProperty(\"first_name\", \"esriFieldTypeString\", \"name\"),\n",
    "        LoadTableProperty(\"last_name\", \"esriFieldTypeString\", \"surname\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Phone\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Phone.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"phone_number\", \"esriFieldTypeString\", \"phoneNo\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Area\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Area.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"area_code\", \"esriFieldTypeString\", \"areaCode\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Crime\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Crime.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"date\", \"esriFieldTypeString\", \"date\"),\n",
    "        LoadTableProperty(\"type\", \"esriFieldTypeString\", \"type\"),\n",
    "        LoadTableProperty(\"last_outcome\", \"esriFieldTypeString\", \"last_outcome\"),\n",
    "        LoadTableProperty(\"note\", \"esriFieldTypeString\", \"note\"),\n",
    "        LoadTableProperty(\"charge\", \"esriFieldTypeString\", \"charge\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Email\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Email.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"email_address\", \"esriFieldTypeString\", \"email_address\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Location\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Location.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"address\", \"esriFieldTypeString\", \"address\"),\n",
    "        LoadTableProperty(\"post_code\", \"esriFieldTypeString\", \"postcode\"),\n",
    "        LoadTableProperty(\"shape\", \"esriFieldTypeGeometry\", [\"latitude:double\",\"longitude:double\"], geometry_type=\"esriGeometryPoint\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Object\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Object.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"description\", \"esriFieldTypeString\", \"description\"),\n",
    "        LoadTableProperty(\"type\", \"esriFieldTypeString\", \"type\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"PhoneCall\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\PhoneCall.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"call_duration\", \"esriFieldTypeInteger\", \"call_duration\"),\n",
    "        LoadTableProperty(\"call_time\", \"esriFieldTypeString\", \"call_time\"),\n",
    "        LoadTableProperty(\"call_date\", \"esriFieldTypeString\", \"call_date\"),\n",
    "        LoadTableProperty(\"call_type\", \"esriFieldTypeString\", \"call_type\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"PostCode\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\PostCode.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"code\", \"esriFieldTypeString\", \"code\")\n",
    "    ]),\n",
    "    LoadTableEntityType(\"Vehicle\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\Vehicle.csv\"), properties=[\n",
    "        LoadTableProperty(\"id\", \"esriFieldTypeInteger\", \":ID\", merge=True),\n",
    "        LoadTableProperty(\"model\", \"esriFieldTypeString\", \"model\"),\n",
    "        LoadTableProperty(\"registration\", \"esriFieldTypeString\", \"reg\"),\n",
    "        LoadTableProperty(\"make\", \"esriFieldTypeString\", \"make\"),\n",
    "        LoadTableProperty(\"year\", \"esriFieldTypeInteger\", \"year\")\n",
    "    ])\n",
    "]\n",
    "relationship_adds = [\n",
    "    LoadTableRelationshipType(\"KNOWS\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\KNOWS.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Person\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"CALLED\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\CALLED.csv\"), RelationshipEndpoint(\"PhoneCall\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Phone\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"CALLER\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\CALLER.csv\"), RelationshipEndpoint(\"PhoneCall\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Phone\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"CURRENT_ADDRESS\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\CURRENT_ADDRESS.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Location\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"FAMILY_REL\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\FAMILY_REL.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Person\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"HAS_EMAIL\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\HAS_EMAIL.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Email\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"HAS_PHONE\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\HAS_PHONE.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Phone\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"HAS_POSTCODE\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\HAS_POSTCODE.csv\"), RelationshipEndpoint(\"Location\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"PostCode\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"INVESTIGATED_BY\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\INVESTIGATED_BY.csv\"), RelationshipEndpoint(\"Crime\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Officer\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"INVOLVED_IN\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\INVOLVED_IN.csv\"), RelationshipEndpoint(\"Vehicle\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Crime\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"INVOLVED_IN\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\INVOLVED_IN.csv\"), RelationshipEndpoint(\"Object\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Crime\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"KNOWS_LW\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\KNOWS_LW.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Person\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"KNOWS_PHONE\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\KNOWS_PHONE.csv\"), RelationshipEndpoint(\"Person\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Person\", \"id\", \":END_ID\", \":ID\")),\n",
    "    LoadTableRelationshipType(\"LOCATION_IN_AREA\", InputTable(r\"C:\\Users\\meg10359\\Downloads\\pole-data-importer\\LOCATION_IN_AREA.csv\"), RelationshipEndpoint(\"Location\", \"id\", \":START_ID\", \":ID\"), RelationshipEndpoint(\"Area\", \"id\", \":END_ID\", \":ID\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad458fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `verify_cert` to False is a security risk, use at your own risk.\n",
      "C:\\ArcGISPro\\bin\\Python\\envs\\new-structure-testing\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "C:\\ArcGISPro\\bin\\Python\\envs\\new-structure-testing\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "C:\\ArcGISPro\\bin\\Python\\envs\\new-structure-testing\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "C:\\ArcGISPro\\bin\\Python\\envs\\new-structure-testing\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "C:\\ArcGISPro\\bin\\Python\\envs\\new-structure-testing\\Lib\\site-packages\\pydantic\\main.py:364: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  Expected `str` but got `LoadTableProperty` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "gis = GIS(\"https://dev0025946.esri.com/portal\",\"publisher2\",\"esri.agp123\",verify_cert=False)\n",
    "created_kg = gis.content.create_service(name=\"TestingLoadNotebook\",service_type=\"KnowledgeGraph\")\n",
    "knowledge_graph = GraphClient(created_kg.url, gis=gis)\n",
    "load_tables(knowledge_graph, entity_adds, relationship_adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a75e7b7a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gis.content.search(\"TestingLoadNotebook\")[0].delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ed517f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
